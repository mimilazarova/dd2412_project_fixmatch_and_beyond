{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FixMatch_training_loop.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmfb90N9fjlY",
        "outputId": "42a7025b-ba5f-4d86-a774-a27df17822ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "pip install tensorflow-addons==0.11.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-addons==0.11.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/51/8e5bb7649ac136292aefef6ea0172d10cc23a26dcda093c62637585bc05e/tensorflow_addons-0.11.1-cp36-cp36m-manylinux2010_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 23.4MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 3.9MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 4.0MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 4.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92kB 4.7MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 4.5MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112kB 4.5MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122kB 4.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133kB 4.5MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143kB 4.5MB/s eta 0:00:01\r\u001b[K     |████▌                           | 153kB 4.5MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 174kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 184kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 204kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 215kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 245kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 256kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 276kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 286kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 307kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 317kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 337kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 348kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 368kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 399kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 409kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 430kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 440kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 460kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 471kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 491kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 501kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 512kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 522kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 532kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 542kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 563kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 573kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 583kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 593kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 604kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 614kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 624kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 634kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 645kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 665kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 675kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 686kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 696kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 706kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 716kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 727kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 737kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 747kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 768kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 778kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 788kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 798kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 808kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 819kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 829kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 839kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 849kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 860kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 870kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 880kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 890kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 901kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 911kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 921kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 942kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 952kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 962kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 972kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 983kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 993kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0MB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0MB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.0MB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.0MB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.1MB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1MB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.1MB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.1MB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1MB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons==0.11.1) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "  Found existing installation: tensorflow-addons 0.8.3\n",
            "    Uninstalling tensorflow-addons-0.8.3:\n",
            "      Successfully uninstalled tensorflow-addons-0.8.3\n",
            "Successfully installed tensorflow-addons-0.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXaHCn3SUuY7"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def regularized_padded_conv(*args, **kwargs):\n",
        "    return tf.keras.layers.Conv2D(*args, **kwargs, padding='same', kernel_regularizer=_regularizer,\n",
        "                                  kernel_initializer='he_normal', use_bias=False)\n",
        "\n",
        "\n",
        "def BN_ReLU(x):\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    return tf.keras.layers.ReLU()(x)\n",
        "\n",
        "\n",
        "def shortcut(x, filters, stride, mode):\n",
        "    if x.shape[-1] == filters:\n",
        "        return x\n",
        "    elif mode == 'B':\n",
        "        return regularized_padded_conv(filters, 1, strides=stride)(x)\n",
        "    elif mode == 'B_original':\n",
        "        x = regularized_padded_conv(filters, 1, strides=stride)(x)\n",
        "        return tf.keras.layers.BatchNormalization()(x)\n",
        "    elif mode == 'A':\n",
        "        return tf.pad(tf.keras.layers.MaxPool2D(1, stride)(x) if stride>1 else x,\n",
        "                      paddings=[(0, 0), (0, 0), (0, 0), (0, filters - x.shape[-1])])\n",
        "    else:\n",
        "        raise KeyError(\"Parameter shortcut_type not recognized!\")\n",
        "    \n",
        "\n",
        "def original_block(x, filters, stride=1, **kwargs):\n",
        "    c1 = regularized_padded_conv(filters, 3, strides=stride)(x)\n",
        "    c2 = regularized_padded_conv(filters, 3)(BN_ReLU(c1))\n",
        "    c2 = tf.keras.layers.BatchNormalization()(c2)\n",
        "    \n",
        "    mode = 'B_original' if _shortcut_type == 'B' else _shortcut_type\n",
        "    x = shortcut(x, filters, stride, mode=mode)\n",
        "    return tf.keras.layers.ReLU()(x + c2)\n",
        "    \n",
        "    \n",
        "def preactivation_block(x, filters, stride=1, preact_block=False):\n",
        "    flow = BN_ReLU(x)\n",
        "    if preact_block:\n",
        "        x = flow\n",
        "        \n",
        "    c1 = regularized_padded_conv(filters, 3, strides=stride)(flow)\n",
        "    if _dropout:\n",
        "        c1 = tf.keras.layers.Dropout(_dropout)(c1)\n",
        "        \n",
        "    c2 = regularized_padded_conv(filters, 3)(BN_ReLU(c1))\n",
        "    x = shortcut(x, filters, stride, mode=_shortcut_type)\n",
        "    return x + c2\n",
        "\n",
        "\n",
        "def bootleneck_block(x, filters, stride=1, preact_block=False):\n",
        "    flow = BN_ReLU(x)\n",
        "    if preact_block:\n",
        "        x = flow\n",
        "         \n",
        "    c1 = regularized_padded_conv(filters//_bootleneck_width, 1)(flow)\n",
        "    c2 = regularized_padded_conv(filters//_bootleneck_width, 3, strides=stride)(BN_ReLU(c1))\n",
        "    c3 = regularized_padded_conv(filters, 1)(BN_ReLU(c2))\n",
        "    x = shortcut(x, filters, stride, mode=_shortcut_type)\n",
        "    return x + c3\n",
        "\n",
        "\n",
        "def group_of_blocks(x, block_type, num_blocks, filters, stride, block_idx=0):\n",
        "    global _preact_shortcuts\n",
        "    preact_block = True if _preact_shortcuts or block_idx == 0 else False\n",
        "    \n",
        "    x = block_type(x, filters, stride, preact_block=preact_block)\n",
        "    for i in range(num_blocks-1):\n",
        "        x = block_type(x, filters)\n",
        "    return x\n",
        "\n",
        "\n",
        "def Resnet(input_shape, n_classes, l2_reg=1e-4, group_sizes=(2, 2, 2), features=(16, 32, 64), strides=(1, 2, 2),\n",
        "           shortcut_type='B', block_type='preactivated', first_conv={\"filters\": 16, \"kernel_size\": 3, \"strides\": 1},\n",
        "           dropout=0, cardinality=1, bootleneck_width=4, preact_shortcuts=True):\n",
        "    \n",
        "    global _regularizer, _shortcut_type, _preact_projection, _dropout, _cardinality, _bootleneck_width, _preact_shortcuts\n",
        "    _bootleneck_width = bootleneck_width # used in ResNeXts and bootleneck blocks\n",
        "    _regularizer = tf.keras.regularizers.l2(l2_reg)\n",
        "    _shortcut_type = shortcut_type # used in blocks\n",
        "    _cardinality = cardinality # used in ResNeXts\n",
        "    _dropout = dropout # used in Wide ResNets\n",
        "    _preact_shortcuts = preact_shortcuts\n",
        "    \n",
        "    block_types = {'preactivated': preactivation_block,\n",
        "                   'bootleneck': bootleneck_block,\n",
        "                   'original': original_block}\n",
        "    \n",
        "    selected_block = block_types[block_type]\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "    flow = regularized_padded_conv(**first_conv)(inputs)\n",
        "    \n",
        "    if block_type == 'original':\n",
        "        flow = BN_ReLU(flow)\n",
        "    \n",
        "    for block_idx, (group_size, feature, stride) in enumerate(zip(group_sizes, features, strides)):\n",
        "        flow = group_of_blocks(flow,\n",
        "                               block_type=selected_block,\n",
        "                               num_blocks=group_size,\n",
        "                               block_idx=block_idx,\n",
        "                               filters=feature,\n",
        "                               stride=stride)\n",
        "    \n",
        "    if block_type != 'original':\n",
        "        flow = BN_ReLU(flow)\n",
        "    \n",
        "    flow = tf.keras.layers.GlobalAveragePooling2D()(flow)\n",
        "    outputs = tf.keras.layers.Dense(n_classes, kernel_regularizer=_regularizer)(flow)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_weights_func(model, model_name):\n",
        "    try: model.load_weights(os.path.join('saved_models', model_name + '.tf'))\n",
        "    except tf.errors.NotFoundError: print(\"No weights found for this model!\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def cifar_wide_resnet(N, K, block_type='preactivated', shortcut_type='B', dropout=0, l2_reg=2.5e-4):\n",
        "    assert (N-4) % 6 == 0, \"N-4 has to be divisible by 6\"\n",
        "    lpb = (N-4) // 6 # layers per block - since N is total number of convolutional layers in Wide ResNet\n",
        "    model = Resnet(input_shape=(32, 32, 3), n_classes=10, l2_reg=l2_reg, group_sizes=(lpb, lpb, lpb), features=(16*K, 32*K, 64*K),\n",
        "                   strides=(1, 2, 2), first_conv={\"filters\": 16, \"kernel_size\": 3, \"strides\": 1}, shortcut_type=shortcut_type,\n",
        "                   block_type=block_type, dropout=dropout, preact_shortcuts=True)\n",
        "    return model\n",
        "\n",
        "\n",
        "def WRN_28_2(shortcut_type='B', load_weights=False, dropout=0, l2_reg=2.5e-4):\n",
        "    model = cifar_wide_resnet(28, 2, 'preactivated', shortcut_type, dropout=dropout, l2_reg=l2_reg)\n",
        "    if load_weights: model = load_weights_func(model, 'cifar_WRN_28_10')\n",
        "    return model\n",
        "\n",
        "\n",
        "def FixMatch():\n",
        "  self.resnet = WRN_28_2()\n",
        "\n",
        "  \"\"\"\n",
        "  Unlabeled data\n",
        "  weak_input = weak_transformation(input)\n",
        "  strong_input = strong_transformation(input)\n",
        "\n",
        "  weak_output = resnet(weak_input)\n",
        "  strong_output = resnet(strong_input)\n",
        "  \n",
        "  pseudo_label = pseudolabel(weak_output)\n",
        "  cross_ent = crossentropy(pseudo_label, strong_output)\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Hfy2W4iGscd"
      },
      "source": [
        "import tensorflow_probability as tfp\n",
        "\n",
        "class CTAugment:\n",
        "  def __init__(self, decay, threshold, depth, n_bins):\n",
        "    self.TRANSFORMATIONS = [ self.autocontrast, self.blur, self.brightness,\n",
        "                          self.color, self.contrast, self.cutout, self.equalize,\n",
        "                          self.invert, self.identity, self.posterize, \n",
        "                          self.rescale, self.rotate, self.sharpness, \n",
        "                          self.shear_x, self.shear_y, self.smooth, \n",
        "                          self.solarize, self.translate_x, self.translate_y]\n",
        "\n",
        "    self.decay = decay\n",
        "    self.threshold = threshold\n",
        "    self.depth = depth\n",
        "    self.n_bins = n_bins\n",
        "  \n",
        "    # we need some way of storing functions so that we can randomly sample\n",
        "    # from them. The format below might not be perfect but it is nice if we \n",
        "    # can generate a index i with which we can access the ith\n",
        "    # transformation, its corresponding bins and their weights\n",
        "    self.bins = [[]]\n",
        "    self.weights = np.ones([len(self.TRANSFORMATIONS), self.n_bins])\n",
        "\n",
        "  def augment(self, x):\n",
        "      for k in range(self.depth):\n",
        "        # we generate a function randomly (here for each sample in batcg)\n",
        "        i = np.random.randint(0, len(self.TRANSFORMATIONS), size=x.shape[0]) \n",
        "\n",
        "        \n",
        "        # pick weights for correpsonding function and set weigths to 0 if they \n",
        "        # are less than 0.8\n",
        "        logits_i = self.weights[i]\n",
        "        threshold_indices = logits_i < self.threshold\n",
        "        logits_i[threshold_indices] = 0\n",
        "\n",
        "        # are the outputs really logits?\n",
        "\n",
        "        # We should probably do this for a whole batch at onces if possible\n",
        "        dist = tfp.distributions.Categorical(logits_i)\n",
        "        bin = dist.sample(1)\n",
        "\n",
        "        # we should probably copy here so we do not overwrite original\n",
        "        x = transformation(x)\n",
        "      \n",
        "      return x\n",
        "\n",
        "\n",
        "  def update_weights(parameter, weights):\n",
        "      pass\n",
        "\n",
        "\n",
        "\n",
        "  def autocontrast():\n",
        "      pass\n",
        "  \n",
        "  def blur():\n",
        "      pass\n",
        "  \n",
        "  def brightness():\n",
        "      pass\n",
        "\n",
        "  def color():\n",
        "      pass\n",
        "\n",
        "  def contrast():\n",
        "      pass\n",
        "\n",
        "  def cutout():\n",
        "      pass\n",
        "\n",
        "  def equalize():\n",
        "      pass\n",
        "\n",
        "  def invert():\n",
        "      pass\n",
        "  \n",
        "  def identity():\n",
        "      pass\n",
        "\n",
        "  def posterize():\n",
        "      pass\n",
        "\n",
        "  def rescale():\n",
        "      pass\n",
        "\n",
        "  def rotate():\n",
        "      pass\n",
        "\n",
        "  def sharpness():\n",
        "      pass\n",
        "\n",
        "  def shear_x():\n",
        "      pass\n",
        "\n",
        "  def shear_y():\n",
        "      pass\n",
        "\n",
        "  def smooth():\n",
        "      pass\n",
        "\n",
        "  def solarize():\n",
        "      pass\n",
        "\n",
        "  def translate_x():\n",
        "      pass\n",
        "\n",
        "  def translate_y():\n",
        "      pass\n"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MY99qVwV6hh"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "logging.getLogger('tensorflow').disabled = True\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_addons as tfa\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "\n",
        "def training(model, ds, mean=None, std=None, lr_values=[0.01, 0.1, 0.01, 0.001], lr_boundaries=[400, 32000, 48000, 64000],\n",
        "                   val_interval=2000, log_interval=200, batch_size=128, nesterov=False):\n",
        "\n",
        "    schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries=lr_boundaries[:-1], values=lr_values)\n",
        "    optimizer = tf.keras.optimizers.SGD(schedule, momentum=0.9, nesterov=nesterov)\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    \n",
        "    def train_prep(x, y):\n",
        "        x = tf.cast(x, tf.float32)/255.\n",
        "        #x = tf.image.random_flip_left_right(x)\n",
        "        #x = tf.image.pad_to_bounding_box(x, 4, 4, 40, 40) #must fix ds independent shape\n",
        "        #x = tf.image.random_crop(x, (32, 32, 3))          #must fix ds independent shape\n",
        "        if mean is not None and std is not None:\n",
        "          x = (x - mean) / std\n",
        "        return x, y\n",
        "\n",
        "    def valid_prep(x, y):\n",
        "        x = tf.cast(x, tf.float32)/255.\n",
        "        if mean is not None and std is not None:\n",
        "          x = (x - mean) / std\n",
        "        return x, y\n",
        "\n",
        "    #ds['train'] = ds['train'].map(train_prep).shuffle(10000).repeat().batch(batch_size).prefetch(-1)\n",
        "    ds['train'] = ds['train'].map(train_prep).batch(batch_size).prefetch(-1)\n",
        "    ds['test'] = ds['test'].map(valid_prep).batch(batch_size*4).prefetch(-1)\n",
        "\n",
        "    #runid = run_name + '_x' + str(np.random.randint(10000))\n",
        "    #writer = tf.summary.create_file_writer(logdir + '/' + runid)\n",
        "    accuracy = tf.metrics.SparseCategoricalAccuracy()\n",
        "    cls_loss = tf.metrics.Mean()\n",
        "    reg_loss = tf.metrics.Mean()\n",
        "    \n",
        "    #print(f\"RUNID: {runid}\")\n",
        "    #tf.keras.utils.plot_model(model)#, os.path.join('saved_plots', runid + '.png'))\n",
        "\n",
        "    def weak_transformation(x):\n",
        "      x = tf.image.random_flip_left_right(x)\n",
        "      max_shift = tf.cast(x.shape[1]*0.125, dtype=tf.dtypes.int32)\n",
        "      shift = tf.random.uniform([x.shape[0], 2], minval=-max_shift, maxval=max_shift, dtype=tf.dtypes.int32)\n",
        "      return tfa.image.translate(x, tf.cast(shift, tf.dtypes.float32))\n",
        "\n",
        "    def strong_transformation(x):\n",
        "      cta = CTAugment(0.99, 0.8, 2, 10)\n",
        "\n",
        "      x_2 = cta.augment(x)\n",
        "      pass\n",
        "\n",
        "    def pseudolabel(class_dist):\n",
        "        argmax = tf.math.argmax(class_dist, axis=1)\n",
        "        return tf.one_hot(argmax, class_dist.shape[1])\n",
        "\n",
        "    def threshold_gate(one_hot, logits, threshold):\n",
        "        max_probs = tf.math.multiply(one_hot, tf.nn.softmax(logits))\n",
        "        return tf.cast(max_probs > threshold, max_probs.dtype)# * max_probs\n",
        "\n",
        "    \n",
        "    @tf.function\n",
        "    def step(x, y, training):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # unlabeled data\n",
        "            x_wk = weak_transformation(x)\n",
        "            outs_wk = model(x_wk, training)  # should this be training or not?\n",
        "            weak_labels = pseudolabel(outs_wk)\n",
        "            weak_labels = threshold_gate(weak_labels, outs_wk, 0.7)\n",
        "\n",
        "            x_str = strong_transformation(x)\n",
        "            #outs_str = model(x_str, training) #should this be training or not?\n",
        "            \n",
        "            #unlabeled_loss = loss_fn(weak_labels, outs_str)\n",
        "            \n",
        "            # labeled data\n",
        "            #outs = model(x, training)\n",
        "            #labeled_loss = loss_fn(y, outs)\n",
        "\n",
        "            #add losses together\n",
        "            #loss = labeled_loss + lambda * unlabeled_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            r_loss = tf.add_n(model.losses)\n",
        "            outs = model(x, training)\n",
        "            c_loss = loss_fn(y, outs)\n",
        "            loss = c_loss + r_loss\n",
        "\n",
        "        if training:\n",
        "            gradients = tape.gradient(loss, model.trainable_weights)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "        accuracy(y, outs)\n",
        "        cls_loss(c_loss)\n",
        "        reg_loss(r_loss)\n",
        "        \n",
        "\n",
        "    training_step = 0\n",
        "    best_validation_acc = 0\n",
        "    epochs = lr_boundaries[-1] // val_interval\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        for x, y in tqdm(ds['train'].take(val_interval), desc=f'epoch {epoch+1}/{epochs}',\n",
        "                         total=val_interval, ncols=100, ascii=True):\n",
        "\n",
        "            training_step += 1\n",
        "            step(x, y, training=True)\n",
        "\n",
        "            if training_step % log_interval == 0:\n",
        "                #with writer.as_default():\n",
        "                    c_loss, r_loss, err = cls_loss.result(), reg_loss.result(), 1-accuracy.result()\n",
        "                    print(f\" c_loss: {c_loss:^6.3f} | r_loss: {r_loss:^6.3f} | err: {err:^6.3f}\", end='\\r')\n",
        "\n",
        "                    tf.summary.scalar('train/error_rate', err, training_step)\n",
        "                    tf.summary.scalar('train/classification_loss', c_loss, training_step)\n",
        "                    tf.summary.scalar('train/regularization_loss', r_loss, training_step)\n",
        "                    tf.summary.scalar('train/learnig_rate', optimizer._decayed_lr('float32'), training_step)\n",
        "                    cls_loss.reset_states()\n",
        "                    reg_loss.reset_states()\n",
        "                    accuracy.reset_states()\n",
        "\n",
        "        for x, y in ds['test']:\n",
        "            step(x, y, training=False)\n",
        "\n",
        "        #with writer.as_default(): TBULATE THE FOLLOWING WHEN UNCOMMENTING!\n",
        "        tf.summary.scalar('test/classification_loss', cls_loss.result(), step=training_step)\n",
        "        tf.summary.scalar('test/error_rate', 1-accuracy.result(), step=training_step)\n",
        "            \n",
        "        if accuracy.result() > best_validation_acc:\n",
        "                best_validation_acc = accuracy.result()\n",
        "                #model.save_weights(os.path.join('saved_models', runid + '.tf'))\n",
        "                \n",
        "        cls_loss.reset_states()\n",
        "        accuracy.reset_states()\n",
        "            \n",
        "    \n",
        "    \n",
        "def cifar_error_test(model, tr_len=20, vd_len=2):\n",
        "\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    optimizer = tf.keras.optimizers.SGD(0.01)\n",
        "\n",
        "    ds = tfds.load('cifar10', as_supervised=True, in_memory=True)\n",
        "    std = tf.reshape((0.2023, 0.1994, 0.2010), shape=(1, 1, 3))\n",
        "    mean= tf.reshape((0.4914, 0.4822, 0.4465), shape=(1, 1, 3))\n",
        "    \n",
        "    def train_prep(x, y):\n",
        "        x = tf.cast(x, tf.float32)/255.\n",
        "        x = tf.image.random_flip_left_right(x)\n",
        "        x = tf.image.pad_to_bounding_box(x, 4, 4, 40, 40)\n",
        "        x = tf.image.random_crop(x, (32, 32, 3))\n",
        "        x = (x - mean) / std\n",
        "        return x, y\n",
        "\n",
        "    def valid_prep(x, y):\n",
        "        x = tf.cast(x, tf.float32)/255.\n",
        "        x = (x - mean) / std\n",
        "        return x, y\n",
        "\n",
        "    ds['train'] = ds['train'].map(train_prep).batch(5).take(tr_len).prefetch(-1)\n",
        "    ds['test'] = ds['test'].map(valid_prep).batch(5).take(vd_len).prefetch(-1)\n",
        "\n",
        "    accuracy = tf.metrics.SparseCategoricalAccuracy()\n",
        "    cls_loss = tf.metrics.Mean()\n",
        "    reg_loss = tf.metrics.Mean()\n",
        "\n",
        "    @tf.function\n",
        "    def step(x, y, training):\n",
        "        with tf.GradientTape() as tape:\n",
        "            r_loss = tf.add_n(model.losses)\n",
        "            outs = model(x, training)\n",
        "            c_loss = loss_fn(y, outs)\n",
        "            loss = c_loss + r_loss\n",
        "\n",
        "        if training:\n",
        "            gradients = tape.gradient(loss, model.trainable_weights)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "        accuracy(y, outs)\n",
        "        cls_loss(c_loss)\n",
        "        reg_loss(r_loss)\n",
        "        \n",
        "    training_step = 0\n",
        "    for x, y in tqdm(ds['train'], desc=f'test', total=tr_len, ncols=100, ascii=True):\n",
        "\n",
        "        training_step += 1\n",
        "        step(x, y, training=True)\n",
        "        c_loss, r_loss, err = cls_loss.result(), reg_loss.result(), 1-accuracy.result()\n",
        "        print(f\" c_loss: {c_loss:^6.3f} | r_loss: {r_loss:^6.3f} | err: {err:^6.3f}\", end='\\r')\n",
        "\n",
        "    for x, y in ds['test']:\n",
        "        step(x, y, training=False)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb9vH0aDt5Ry",
        "outputId": "6b1e6d57-280d-4549-c769-eb93e4b6fc68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "ds = tfds.load('cifar10', as_supervised=True, in_memory=True)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:`in_memory` if deprecated and will be removed in a future version. Please use `ds = ds.cache()` instead.\n",
            "WARNING:absl:`in_memory` if deprecated and will be removed in a future version. Please use `ds = ds.cache()` instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h06AqVwlXATf"
      },
      "source": [
        "model = WRN_28_2()"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIpVOZA3V7Wl",
        "outputId": "ffbef0a8-9a20-46c6-9fb4-b25006a8d39a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        }
      },
      "source": [
        "training(model, ds)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1/32:   0%|                                                          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(128, 10)\n",
            "Tensor(\"Categorical_1/sample/Reshape_2:0\", shape=(1, 128), dtype=int32)\n",
            "(1, 128)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-d8a7046d3255>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-88-73e247d023b1>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, ds, mean, std, lr_values, lr_boundaries, val_interval, log_interval, batch_size, nesterov)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mtraining_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtraining_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: in user code:\n\n    <ipython-input-83-73e247d023b1>:78 step  *\n        x_str = strong_transformation(x)\n    <ipython-input-83-73e247d023b1>:57 strong_transformation  *\n        x_2 = cta.augment(x)\n    <ipython-input-87-fe063885c196>:48 augment  *\n        x = transformation(x)\n\n    NameError: name 'transformation' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bkYlQ3BbM5A"
      },
      "source": [
        "#ds = tfds.load('cifar10', as_supervised=True, in_memory=True)\n",
        "\n",
        "mean = {\n",
        "'cifar10': tf.reshape((0.4914, 0.4822, 0.4465), shape=(1, 1, 3)),\n",
        "'cifar100': tf.reshape((0.5071, 0.4867, 0.4408), shape=(1, 1, 3)),\n",
        "}\n",
        "\n",
        "std = {\n",
        "'cifar10': tf.reshape((0.2023, 0.1994, 0.2010), shape=(1, 1, 3)),\n",
        "'cifar100': tf.reshape((0.2675, 0.2565, 0.2761), shape=(1, 1, 3))\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_ET93aSebHs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}