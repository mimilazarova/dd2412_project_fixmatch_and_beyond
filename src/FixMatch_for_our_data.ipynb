{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FixMatch_for_our_data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmfb90N9fjlY",
        "outputId": "8cea8345-b753-46ba-b27d-b284144a654f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "pip install tensorflow-addons==0.11.1"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-addons==0.11.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/51/8e5bb7649ac136292aefef6ea0172d10cc23a26dcda093c62637585bc05e/tensorflow_addons-0.11.1-cp36-cp36m-manylinux2010_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons==0.11.1) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "  Found existing installation: tensorflow-addons 0.8.3\n",
            "    Uninstalling tensorflow-addons-0.8.3:\n",
            "      Successfully uninstalled tensorflow-addons-0.8.3\n",
            "Successfully installed tensorflow-addons-0.11.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow_addons"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXaHCn3SUuY7"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def regularized_padded_conv(*args, **kwargs):\n",
        "    return tf.keras.layers.Conv2D(*args, **kwargs, padding='same', kernel_regularizer=_regularizer,\n",
        "                                  kernel_initializer='he_normal', use_bias=False)\n",
        "\n",
        "\n",
        "def BN_ReLU(x):\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    return tf.keras.layers.ReLU()(x)\n",
        "\n",
        "\n",
        "def shortcut(x, filters, stride, mode):\n",
        "    if x.shape[-1] == filters:\n",
        "        return x\n",
        "    elif mode == 'B':\n",
        "        return regularized_padded_conv(filters, 1, strides=stride)(x)\n",
        "    elif mode == 'B_original':\n",
        "        x = regularized_padded_conv(filters, 1, strides=stride)(x)\n",
        "        return tf.keras.layers.BatchNormalization()(x)\n",
        "    elif mode == 'A':\n",
        "        return tf.pad(tf.keras.layers.MaxPool2D(1, stride)(x) if stride>1 else x,\n",
        "                      paddings=[(0, 0), (0, 0), (0, 0), (0, filters - x.shape[-1])])\n",
        "    else:\n",
        "        raise KeyError(\"Parameter shortcut_type not recognized!\")\n",
        "    \n",
        "\n",
        "def original_block(x, filters, stride=1, **kwargs):\n",
        "    c1 = regularized_padded_conv(filters, 3, strides=stride)(x)\n",
        "    c2 = regularized_padded_conv(filters, 3)(BN_ReLU(c1))\n",
        "    c2 = tf.keras.layers.BatchNormalization()(c2)\n",
        "    \n",
        "    mode = 'B_original' if _shortcut_type == 'B' else _shortcut_type\n",
        "    x = shortcut(x, filters, stride, mode=mode)\n",
        "    return tf.keras.layers.ReLU()(x + c2)\n",
        "    \n",
        "    \n",
        "def preactivation_block(x, filters, stride=1, preact_block=False):\n",
        "    flow = BN_ReLU(x)\n",
        "    if preact_block:\n",
        "        x = flow\n",
        "        \n",
        "    c1 = regularized_padded_conv(filters, 3, strides=stride)(flow)\n",
        "    if _dropout:\n",
        "        c1 = tf.keras.layers.Dropout(_dropout)(c1)\n",
        "        \n",
        "    c2 = regularized_padded_conv(filters, 3)(BN_ReLU(c1))\n",
        "    x = shortcut(x, filters, stride, mode=_shortcut_type)\n",
        "    return x + c2\n",
        "\n",
        "\n",
        "def bootleneck_block(x, filters, stride=1, preact_block=False):\n",
        "    flow = BN_ReLU(x)\n",
        "    if preact_block:\n",
        "        x = flow\n",
        "         \n",
        "    c1 = regularized_padded_conv(filters//_bootleneck_width, 1)(flow)\n",
        "    c2 = regularized_padded_conv(filters//_bootleneck_width, 3, strides=stride)(BN_ReLU(c1))\n",
        "    c3 = regularized_padded_conv(filters, 1)(BN_ReLU(c2))\n",
        "    x = shortcut(x, filters, stride, mode=_shortcut_type)\n",
        "    return x + c3\n",
        "\n",
        "\n",
        "def group_of_blocks(x, block_type, num_blocks, filters, stride, block_idx=0):\n",
        "    global _preact_shortcuts\n",
        "    preact_block = True if _preact_shortcuts or block_idx == 0 else False\n",
        "    \n",
        "    x = block_type(x, filters, stride, preact_block=preact_block)\n",
        "    for i in range(num_blocks-1):\n",
        "        x = block_type(x, filters)\n",
        "    return x\n",
        "\n",
        "\n",
        "def Resnet(input_shape, n_classes, l2_reg=1e-4, group_sizes=(2, 2, 2), features=(16, 32, 64), strides=(1, 2, 2),\n",
        "           shortcut_type='B', block_type='preactivated', first_conv={\"filters\": 16, \"kernel_size\": 3, \"strides\": 1},\n",
        "           dropout=0, cardinality=1, bootleneck_width=4, preact_shortcuts=True):\n",
        "    \n",
        "    global _regularizer, _shortcut_type, _preact_projection, _dropout, _cardinality, _bootleneck_width, _preact_shortcuts\n",
        "    _bootleneck_width = bootleneck_width # used in ResNeXts and bootleneck blocks\n",
        "    _regularizer = tf.keras.regularizers.l2(l2_reg)\n",
        "    _shortcut_type = shortcut_type # used in blocks\n",
        "    _cardinality = cardinality # used in ResNeXts\n",
        "    _dropout = dropout # used in Wide ResNets\n",
        "    _preact_shortcuts = preact_shortcuts\n",
        "    \n",
        "    block_types = {'preactivated': preactivation_block,\n",
        "                   'bootleneck': bootleneck_block,\n",
        "                   'original': original_block}\n",
        "    \n",
        "    selected_block = block_types[block_type]\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "    flow = regularized_padded_conv(**first_conv)(inputs)\n",
        "    \n",
        "    if block_type == 'original':\n",
        "        flow = BN_ReLU(flow)\n",
        "    \n",
        "    for block_idx, (group_size, feature, stride) in enumerate(zip(group_sizes, features, strides)):\n",
        "        flow = group_of_blocks(flow,\n",
        "                               block_type=selected_block,\n",
        "                               num_blocks=group_size,\n",
        "                               block_idx=block_idx,\n",
        "                               filters=feature,\n",
        "                               stride=stride)\n",
        "    \n",
        "    if block_type != 'original':\n",
        "        flow = BN_ReLU(flow)\n",
        "    \n",
        "    flow = tf.keras.layers.GlobalAveragePooling2D()(flow)\n",
        "    outputs = tf.keras.layers.Dense(n_classes, kernel_regularizer=_regularizer)(flow)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_weights_func(model, model_name):\n",
        "    try: model.load_weights(os.path.join('saved_models', model_name + '.tf'))\n",
        "    except tf.errors.NotFoundError: print(\"No weights found for this model!\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def cifar_wide_resnet(N, K, block_type='preactivated', shortcut_type='B', dropout=0, l2_reg=2.5e-4):\n",
        "    assert (N-4) % 6 == 0, \"N-4 has to be divisible by 6\"\n",
        "    lpb = (N-4) // 6 # layers per block - since N is total number of convolutional layers in Wide ResNet\n",
        "    model = Resnet(input_shape=(32, 32, 3), n_classes=10, l2_reg=l2_reg, group_sizes=(lpb, lpb, lpb), features=(16*K, 32*K, 64*K),\n",
        "                   strides=(1, 2, 2), first_conv={\"filters\": 16, \"kernel_size\": 3, \"strides\": 1}, shortcut_type=shortcut_type,\n",
        "                   block_type=block_type, dropout=dropout, preact_shortcuts=True)\n",
        "    return model\n",
        "\n",
        "\n",
        "def WRN_28_2(shortcut_type='B', load_weights=False, dropout=0, l2_reg=2.5e-4):\n",
        "    model = cifar_wide_resnet(28, 2, 'preactivated', shortcut_type, dropout=dropout, l2_reg=l2_reg)\n",
        "    if load_weights: model = load_weights_func(model, 'cifar_WRN_28_10')\n",
        "    return model"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Hfy2W4iGscd"
      },
      "source": [
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps, ImageEnhance, ImageFilter\n",
        "\n",
        "\n",
        "class CTAugment:\n",
        "\n",
        "  def __init__(self, n_classes, decay=0.99, threshold=0.85, depth=2, n_bins=17):\n",
        "    self.decay = decay\n",
        "    self.threshold = threshold\n",
        "    self.depth = depth\n",
        "    self.n_bins = n_bins\n",
        "    self.n_classes = n_classes\n",
        "  \n",
        "    # we need some way of storing functions so that we can randomly sample\n",
        "    # from them. The format below might not be perfect but it is nice if we \n",
        "    # can generate a index i wish with which we can access the ith\n",
        "    # transformation, its corresponding bins and their weights\n",
        "    self.xforms = []\n",
        "    self.bins = [[]]\n",
        "    self.weights = [[]]\n",
        "\n",
        "    self.AUG_DICT = {\n",
        "        \"autocontrast\": {\"f\": self.autocontrast, \"weight\": [np.ones(self.n_bins)*1.0]},\n",
        "        \"blur\": {\"f\": self.blur, \"weight\": [np.ones(self.n_bins)*1.0]},\n",
        "        \"brightness\": {\"f\": self.brightness, \"weight\":[np.ones(self.n_bins)*1.0]},\n",
        "        \"color\": {\"f\": self.color, \"weight\": [np.ones(self.n_bins)*1.0]},\n",
        "        \"contrast\": {\"f\": self.contrast, \"weight\": [np.ones(self.n_bins)*1.0]},\n",
        "        \"cutout\": {\"f\": self.cutout, \"weight\": [np.ones(self.n_bins)*1.0]},\n",
        "        \"equalize\": {\"f\": self.equalize, \"weight\": [np.ones(self.n_bins)*1.0]},\n",
        "        \"invert\": {\"f\": self.invert, \"weight\": [np.ones(self.n_bins)*1.0]},\n",
        "        \"identity\": {\"f\": self.identity, \"weight\": [np.ones(self.n_bins)*1.0]},\n",
        "        \"posterize\": {\"f\": self.posterize, \"weight\": [np.ones(self.n_bins)*1.0]},\n",
        "        \"rescale\": {\"f\": self.rescale, \"weight\": [np.ones(self.n_bins)*1.0, np.ones(6)*1.0]},\n",
        "        \"rotate\": {\"f\": self.rotate, \"weight\": [np.ones(self.n_bins)*1.0]},\n",
        "        \"sharpness\": {\"f\": self.sharpness, \"weight\": [np.ones(self.n_bins)*1.0]},\n",
        "        \"shear_x\": {\"f\": self.shear_x, \"weight\": [np.ones(self.n_bins)*1.0]},\n",
        "        \"shear_y\": {\"f\": self.shear_y, \"weight\": [np.ones(self.n_bins)*1.0]},\n",
        "        \"smooth\": {\"f\": self.smooth, \"weight\": [np.ones(self.n_bins)*1.0]},\n",
        "        \"solarize\": {\"f\": self.solarize, \"weight\": [np.ones(self.n_bins)*1.0]},\n",
        "        \"translate_x\": {\"f\": self.translate_x, \"weight\": [np.ones(self.n_bins)*1.0]},\n",
        "        \"translate_y\": {\"f\": self.translate_y, \"weight\": [np.ones(self.n_bins)*1.0]}\n",
        "    }\n",
        "    self.N = len(self.AUG_DICT.keys())\n",
        "    self.options = list(self.AUG_DICT.keys())\n",
        "\n",
        "    self.batch_choices = []\n",
        "    self.batch_bins = []\n",
        "\n",
        "  def weight_to_p(self, weight):\n",
        "        p = weight + (1 - self.decay)  # Avoid to have all zero.\n",
        "        p = p / p.max()\n",
        "        p[p < self.threshold] = 0\n",
        "        return p/np.sum(p)\n",
        "\n",
        "  def augment(self, x):\n",
        "    aug_x = Image.fromarray(np.uint8(255*x))\n",
        "\n",
        "\n",
        "    choices = [self.options[i] for i in np.random.choice(np.arange(self.N), self.depth, replace=False)]\n",
        "    bins = []\n",
        "\n",
        "    for k in range(self.depth):\n",
        "        choice_key = choices[k]\n",
        "        \n",
        "        transformation = self.AUG_DICT[choice_key][\"f\"]\n",
        "        # pick weights for correpsonding function and set weigths to 0 if they \n",
        "        # are less than 0.8\n",
        "        w = self.AUG_DICT[choice_key][\"weight\"][0]\n",
        "        p = self.weight_to_p(w)\n",
        "        curr_bins = {}\n",
        "        curr_bins[\"bin\"] = np.random.choice(np.arange(self.n_bins), p=p)\n",
        "\n",
        "        if choice_key==\"rescale\":\n",
        "          w = self.AUG_DICT[choice_key][\"weight\"][1]\n",
        "          p = self.weight_to_p(w)\n",
        "          curr_bins[\"bin2\"] = np.random.choice(np.arange(6), p=p)\n",
        "\n",
        "        # we should probably copy here so we do not overwrite original\n",
        "        aug_x = transformation(aug_x, **curr_bins)\n",
        "        bins.append(curr_bins)\n",
        "\n",
        "    return np.array(aug_x), choices, bins\n",
        "\n",
        "  def augment_batch(self, batch):\n",
        "    aug_batch = tf.identity(batch)\n",
        "\n",
        "    #aug_batch = tf.map_fn(aug_batch, self.augment)\n",
        "    batch_choices = []\n",
        "    batch_bins = []\n",
        "    \n",
        "    if batch.ndim == 3:\n",
        "      sample, choices, bins = self.augment(sample)\n",
        "      batch_choices.append(choices)\n",
        "      batch_bins.append(bins)\n",
        "    elif batch.ndim == 4:\n",
        "      for sample in aug_batch:\n",
        "        sample, choices, bins = self.augment(sample)\n",
        "        batch_choices.append(choices)\n",
        "        batch_bins.append(bins)\n",
        "\n",
        "    return aug_batch, batch_choices, batch_bins\n",
        "\n",
        "  def update_weights(self, label, pred, choices, bins):\n",
        "    omega = 1 - 1 / (2*self.n_classes) * np.sum(tf.math.abs(label - pred))\n",
        "\n",
        "    for k in range(self.depth):\n",
        "\n",
        "      w = self.AUG_DICT[choices[k]][\"weight\"][0]\n",
        "      #tmp = np.copy(w)\n",
        "      w[bins[k][\"bin\"]] = self.decay * w[bins[k][\"bin\"]] + (1 - self.decay) * omega\n",
        "      #print(tmp-w)\n",
        "      if choices[k] == \"rescale\":\n",
        "        w = self.AUG_DICT[choices[k]][\"weight\"][1]\n",
        "        #tmp = np.copy(w)\n",
        "        w[bins[k][\"bin2\"]] = self.decay * w[bins[k][\"bin2\"]] + (1 - self.decay) * omega\n",
        "        #print(tmp-w)\n",
        "\n",
        "\n",
        "  def update_weights_batch(self, labels, preds, choices, bins):\n",
        "    [self.update_weights(l, p, c, b) for l, p, c, b in zip(labels, preds, choices, bins)]\n",
        "\n",
        "  def get_param(self, r_min, r_max, bin):\n",
        "      possible_value = np.linspace(r_min, r_max, self.n_bins)\n",
        "      return possible_value[bin]\n",
        "\n",
        "  def autocontrast(self, x, bin):\n",
        "      param = self.get_param(0, 1, bin)\n",
        "      return Image.blend(x, ImageOps.autocontrast(x), param)\n",
        "  \n",
        "  def blur(self, x, bin):\n",
        "      param = self.get_param(0, 1, bin)\n",
        "      return Image.blend(x, x.filter(ImageFilter.BLUR), param)\n",
        "  \n",
        "  def brightness(self, x, bin):\n",
        "      param = self.get_param(0, 1, bin)\n",
        "      return ImageEnhance.Brightness(x).enhance(0.1 + 1.9*param)\n",
        "\n",
        "  def color(self, x, bin):\n",
        "      param = self.get_param(0, 1, bin)\n",
        "      return ImageEnhance.Color(x).enhance(0.1 + 1.9*param)\n",
        "\n",
        "  def contrast(self, x, bin):\n",
        "      param = self.get_param(0, 1, bin)\n",
        "      return ImageEnhance.Contrast(x).enhance(0.1 + 1.9*param)\n",
        "\n",
        "  def cutout(self, x, bin):\n",
        "    \"\"\"Taken directlly from FixMatch code\"\"\"\n",
        "    level = self.get_param(0, 0.5, bin)\n",
        "\n",
        "    size = 1 + int(level * min(x.size) * 0.499)\n",
        "    img_height, img_width = x.size\n",
        "    height_loc = np.random.randint(low=0, high=img_height)\n",
        "    width_loc = np.random.randint(low=0, high=img_width)\n",
        "    upper_coord = (max(0, height_loc - size // 2), max(0, width_loc - size // 2))\n",
        "    lower_coord = (min(img_height, height_loc + size // 2), min(img_width, width_loc + size // 2))\n",
        "    pixels = x.load()  # create the pixel map\n",
        "    for i in range(upper_coord[0], lower_coord[0]):  # for every col:\n",
        "        for j in range(upper_coord[1], lower_coord[1]):  # For every row\n",
        "            pixels[i, j] = (127, 127, 127)  # set the color accordingly\n",
        "    return x\n",
        "\n",
        "  def equalize(self, x, bin):\n",
        "      param = self.get_param(0, 1, bin)\n",
        "      return Image.blend(x, ImageOps.equalize(x), param)\n",
        "\n",
        "  def invert(self, x, bin):\n",
        "      param = self.get_param(0, 1, bin)\n",
        "      return Image.blend(x, ImageOps.invert(x), param)\n",
        "  \n",
        "  def identity(self, x, bin):\n",
        "      return x\n",
        "\n",
        "  def posterize(self, x, bin):\n",
        "      param = int(self.get_param(0, 8, bin))\n",
        "      return ImageOps.posterize(x, param)\n",
        "\n",
        "  def rescale(self, x, bin, bin2):\n",
        "      param = self.get_param(0.5, 1, bin)\n",
        "      methods = (Image.ANTIALIAS, Image.BICUBIC, Image.BILINEAR, Image.BOX, Image.HAMMING, Image.NEAREST)\n",
        "      method = methods[bin2]\n",
        "      s = x.size\n",
        "      scale = param*0.25\n",
        "      crop = (scale * s[0], scale * s[1], s[0] * (1 - scale), s[1] * (1 - scale))\n",
        "      return x.crop(crop).resize(x.size, method)\n",
        "\n",
        "  def rotate(self, x, bin):\n",
        "      param = self.get_param(-45, 45, bin)\n",
        "      angle = int(np.round((2 * param - 1) * 45))\n",
        "      return x.rotate(angle)\n",
        "\n",
        "  def sharpness(self, x, bin):\n",
        "      param = self.get_param(0, 1, bin)\n",
        "      return ImageEnhance.Sharpness(x).enhance(0.1 + 1.9*param)\n",
        "\n",
        "  def shear_x(self, x, bin):\n",
        "      param = self.get_param(-0.3, 0.3, bin)\n",
        "      shear = (2 * param - 1) * 0.3\n",
        "      return x.transform(x.size, Image.AFFINE, (1, shear, 0, 0, 1, 0))\n",
        "\n",
        "  def shear_y(self, x, bin):\n",
        "      param = self.get_param(-0.3, 0.3, bin)\n",
        "      shear = (2 * param - 1) * 0.3\n",
        "      return x.transform(x.size, Image.AFFINE, (1, 0, 0, shear, 1, 0))\n",
        "\n",
        "  def smooth(self, x, bin):\n",
        "      param = self.get_param(0, 1, bin)\n",
        "      return Image.blend(x, x.filter(ImageFilter.SMOOTH), param)\n",
        "\n",
        "  def solarize(self, x, bin):\n",
        "      param = self.get_param(0, 1, bin)\n",
        "      th = int(param * 255.999)\n",
        "      return ImageOps.solarize(x, th)\n",
        "\n",
        "  def translate_x(self, x, bin):\n",
        "      param = self.get_param(-0.3, 0.3, bin)\n",
        "      delta = (2 * param - 1) * 0.3\n",
        "      return x.transform(x.size, Image.AFFINE, (1, 0, delta, 0, 1, 0))\n",
        "\n",
        "  def translate_y(self, x, bin):\n",
        "      param = self.get_param(-0.3, 0.3, bin)\n",
        "      delta = (2 * param - 1) * 0.3\n",
        "      return x.transform(x.size, Image.AFFINE, (1, 0, 0, 0, 1, delta))"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9nFO9BDTOXD"
      },
      "source": [
        "class OurCosineDecay(tf.keras.experimental.CosineDecay):\n",
        "\n",
        "  def __call__(self, step):\n",
        "    print(\"HEJ\")\n",
        "    with ops.name_scope_v2(self.name or \"CosineDecay\"):\n",
        "      initial_learning_rate = ops.convert_to_tensor_v2(\n",
        "          self.initial_learning_rate, name=\"initial_learning_rate\")\n",
        "      dtype = initial_learning_rate.dtype\n",
        "      decay_steps = math_ops.cast(self.decay_steps, dtype)\n",
        "\n",
        "      global_step_recomp = math_ops.cast(step, dtype)\n",
        "      global_step_recomp = math_ops.minimum(global_step_recomp, decay_steps)\n",
        "      completed_fraction = global_step_recomp / decay_steps\n",
        "      cosine_decayed = math_ops.cos(\n",
        "          constant_op.constant(7/16 * math.pi) * completed_fraction)\n",
        "\n",
        "      decayed = (1 - self.alpha) * cosine_decayed + self.alpha\n",
        "      return math_ops.multiply(initial_learning_rate, decayed)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MY99qVwV6hh"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "logging.getLogger('tensorflow').disabled = True\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_addons as tfa\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "\n",
        "def training(model, ds_l, ds_u, hparams, mean=None, std=None,\n",
        "                   val_interval=2000, log_interval=200, batch_size=128):\n",
        "\n",
        "    def train_prep(x, y):\n",
        "        x = tf.cast(x, tf.float32)/255.\n",
        "        return x, y\n",
        "\n",
        "    def valid_prep(x, y):\n",
        "        x = tf.cast(x, tf.float32)/255.\n",
        "        return x, y\n",
        "\n",
        "    def weak_transformation(x):\n",
        "      x = tf.image.random_flip_left_right(x)\n",
        "      max_shift = tf.cast(x.shape[1]*0.125, dtype=tf.dtypes.int32)\n",
        "      shift = tf.random.uniform([x.shape[0], 2], minval=-max_shift, maxval=max_shift, dtype=tf.dtypes.int32)\n",
        "      \n",
        "      return tfa.image.translate(x, tf.cast(shift, tf.dtypes.float32))\n",
        "      \n",
        "\n",
        "    def pseudolabel(class_dist):\n",
        "        argmax = tf.math.argmax(class_dist, axis=1)\n",
        "        return tf.one_hot(argmax, class_dist.shape[1])\n",
        "\n",
        "    def threshold_gate(one_hot, logits, threshold):\n",
        "        max_probs = tf.math.multiply(one_hot, tf.nn.softmax(logits))\n",
        "        return tf.cast(max_probs > threshold, max_probs.dtype)# * max_probs\n",
        "\n",
        "    \n",
        "    #@tf.function\n",
        "    def step(x_l, y_l, x_u, training):\n",
        "        with tf.GradientTape() as tape:            \n",
        "\n",
        "            # labeled data\n",
        "            x_l_weak = weak_transformation(x_l)\n",
        "            output_l = model(x_l_weak, training)\n",
        "            loss_l = loss_fn(y_l, output_l)\n",
        "\n",
        "            \n",
        "            # unlabeled data\n",
        "            x_u_weak = weak_transformation(x_u)\n",
        "            output_u_weak = model(x_u_weak, training)  # should this be training or not?\n",
        "            y_u = pseudolabel(output_u_weak)\n",
        "            y_u = threshold_gate(y_u, output_u_weak, hparams['treshold'])\n",
        "\n",
        "            x_u_strong, choices, bins = cta.augment_batch(x)\n",
        "            output_u_strong = model(x_u_strong, training)\n",
        "            cta.update_weights_batch(y_u, outputs_u_strong, choices, bins)\n",
        "            \n",
        "            unlabeled_loss = loss_fn(y_u, output_u_strong)\n",
        "            \n",
        "\n",
        "            #add losses together\n",
        "            loss = labeled_loss + hparams['lamda'] * unlabeled_loss\n",
        "\n",
        "\n",
        "        if training:\n",
        "            gradients = tape.gradient(loss, model.trainable_weights)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "        accuracy(y, outs)\n",
        "        cls_loss(c_loss)\n",
        "        reg_loss(r_loss)\n",
        "\n",
        "    schedule = OurCosineDecay(hparams['eta'], hparams['K'])\n",
        "    optimizer = tf.keras.optimizers.SGD(schedule, momentum=hparams['beta'], nesterov=hparams['nesterov'])\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    cta = CTAugment(hparams['cta_classes'], hparams['cta_decay'], hparams['cta_threshold'], hparams['cta_depth'])\n",
        "    \n",
        "    # split into batches\n",
        "    #img_l = tf.split(img_l, hparams['batch_size'], axis=0, name='split')\n",
        "    ds_l = ds_l.map(train_prep).batch(hparams['batch_size']).prefetch(-1)\n",
        "    ds_u = ds_u.map(train_prep).batch(hparams['batch_size']).prefetch(-1)\n",
        "\n",
        "\n",
        "    #runid = run_name + '_x' + str(np.random.randint(10000))\n",
        "    #writer = tf.summary.create_file_writer(logdir + '/' + runid)\n",
        "    accuracy = tf.metrics.SparseCategoricalAccuracy()\n",
        "    cls_loss = tf.metrics.Mean()\n",
        "    reg_loss = tf.metrics.Mean()\n",
        "    \n",
        "    #print(f\"RUNID: {runid}\")\n",
        "    #tf.keras.utils.plot_model(model)#, os.path.join('saved_plots', runid + '.png'))    \n",
        "\n",
        "    training_step = 0\n",
        "    best_validation_acc = 0\n",
        "    epochs = 1\n",
        "\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        #for x, y in tqdm( ds_l.take(val_interval), desc=f'epoch {epoch+1}/{epochs}',\n",
        "        #                 total=val_interval, ncols=100, ascii=True):\n",
        "        for (x_l, y_l), (x_u, _) in tqdm( zip(ds_l, ds_u), desc=f'epoch {epoch+1}/{epochs}',\n",
        "                         total=val_interval, ncols=100, ascii=True):\n",
        "\n",
        "            tf.print(x_l.shape)\n",
        "            tf.print(y_l.shape)            \n",
        "            tf.print(x_u.shape)\n",
        "\n",
        "\n",
        "            training_step += 1\n",
        "            step(x_l, y_l, x_u, training=True)\n",
        "\n",
        "            if training_step % log_interval == 0:\n",
        "                #with writer.as_default():\n",
        "                    c_loss, r_loss, err = cls_loss.result(), reg_loss.result(), 1-accuracy.result()\n",
        "                    print(f\" c_loss: {c_loss:^6.3f} | r_loss: {r_loss:^6.3f} | err: {err:^6.3f}\", end='\\r')\n",
        "\n",
        "                    tf.summary.scalar('train/error_rate', err, training_step)\n",
        "                    tf.summary.scalar('train/classification_loss', c_loss, training_step)\n",
        "                    tf.summary.scalar('train/regularization_loss', r_loss, training_step)\n",
        "                    tf.summary.scalar('train/learnig_rate', optimizer._decayed_lr('float32'), training_step)\n",
        "                    cls_loss.reset_states()\n",
        "                    reg_loss.reset_states()\n",
        "                    accuracy.reset_states()\n",
        "\n",
        "        for x, y in ds['test']:\n",
        "            step(x, y, training=False)\n",
        "\n",
        "        #with writer.as_default(): TBULATE THE FOLLOWING WHEN UNCOMMENTING!\n",
        "        tf.summary.scalar('test/classification_loss', cls_loss.result(), step=training_step)\n",
        "        tf.summary.scalar('test/error_rate', 1-accuracy.result(), step=training_step)\n",
        "            \n",
        "        if accuracy.result() > best_validation_acc:\n",
        "                best_validation_acc = accuracy.result()\n",
        "                #model.save_weights(os.path.join('saved_models', runid + '.tf'))\n",
        "                \n",
        "        cls_loss.reset_states()\n",
        "        accuracy.reset_states()\n",
        "            "
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb9vH0aDt5Ry",
        "outputId": "eb7d1443-91f6-4f6a-ddba-56e4cee3183d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "ds = tfds.load('cifar10', as_supervised=True)\n",
        "\n",
        "print(ds['train'])\n",
        "print(type(ds['train']))"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<DatasetV1Adapter shapes: ((32, 32, 3), ()), types: (tf.uint8, tf.int64)>\n",
            "<class 'tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZo0cMs3c2FD",
        "outputId": "12cce1b2-b2f4-4f10-e424-5c0a9b789f30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "\n",
        "print(os.path.exists('cifar10.3@10-label.json'))\n",
        "#ds_l, ds_u, ls = load_all('', 'cifar10', 3, 10)\n",
        "ds_l = LoadData('cifar10.3@10-label.tfrecord', tensor=True)\n",
        "\n",
        "ds_u = LoadData('cifar10.3@10-label.tfrecord', tensor=True)"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h06AqVwlXATf"
      },
      "source": [
        "model = WRN_28_2()"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIpVOZA3V7Wl"
      },
      "source": [
        "# hyperparams\n",
        "lamda = 1     # proportion of unlabeled loss in total loss\n",
        "eta = 0.03    # learning rate\n",
        "beta = 0.09   # momentum\n",
        "tau = 0.95    # threshold in pseudo-labeling\n",
        "mu = 0.7      # proportion of unlabeled samples in batch\n",
        "B = 64        # number of labeled examples in batch(in training)\n",
        "K = 2 ** 20\n",
        "nesterov = False\n",
        "batch_size = 2  # should be 64?\n",
        "# weight decay\n",
        "# SGD instead of Adam\n",
        "\n",
        "\n",
        "#CTAugment params\n",
        "cta_classes = 10\n",
        "cta_decay = 0.99\n",
        "cta_depth = 2\n",
        "cta_threshold = 0.8\n",
        "\n",
        "hparams = {'lamda': lamda, 'eta': eta, 'beta': beta, 'tau': tau, 'mu': mu, 'B': B, 'K': K, 'nesterov': False, 'batch_size': batch_size,\n",
        "           'cta_classes': cta_classes, 'cta_decay': cta_decay, 'cta_depth': cta_depth, 'cta_threshold': cta_threshold}"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLqOJ6-yVFgc",
        "outputId": "7abe79d0-037f-4994-8844-e1bc477793bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "training(model, ds_l, ds_u, hparams)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\repoch 1/1:   0%|                                                           | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TensorShape([2, 32, 32, 3])\n",
            "TensorShape([2])\n",
            "TensorShape([2, 32, 32, 3])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\repoch 1/1:   0%|                                                           | 0/2000 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-170-8b743aeff31f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-166-a6d48eb422d5>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, ds_l, ds_u, hparams, mean, std, val_interval, log_interval, batch_size)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mtraining_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtraining_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-166-a6d48eb422d5>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(x_l, y_l, x_u, training)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# labeled data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mx_l_weak\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_transformation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0moutput_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_l_weak\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mloss_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-166-a6d48eb422d5>\u001b[0m in \u001b[0;36mweak_transformation\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mmax_shift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.125\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0mshift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmax_shift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_shift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n\n    TypeError: tf__translations_to_projective_transforms() takes from 0 to 2 positional arguments but 4 were given\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDkADDNyxZjn"
      },
      "source": [
        "import os\n",
        "\n",
        "def log_run(path, file, hparams, results):\n",
        "\n",
        "  values = list(hparams.values())\n",
        "  values += list(results.values())\n",
        "\n",
        "\n",
        "  if os.path.exists(filename + '.csv'):\n",
        "    append_write = 'a' # append if already exists\n",
        "    log_book = open(filename,append_write)\n",
        "  else:\n",
        "    append_write = 'w' # make a new file if not\n",
        "\n",
        "    columns = list(hparams.keys())\n",
        "    columns += list(results.keys())\n",
        "\n",
        "    log_book = open(filename,append_write)\n",
        "\n",
        "    columns_string = ','.join(str(c) for c in columns)\n",
        "    log_book.write(columns_string + '\\n')\n",
        "\n",
        "  values_string = ','.join(str(v) for v in values)\n",
        "\n",
        "  log_book.write(values_string + '\\n')\n",
        "  log_book.close()"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bkYlQ3BbM5A"
      },
      "source": [
        "  #ds = tfds.load('cifar10', as_supervised=True, in_memory=True)\n",
        "\n",
        "mean = {\n",
        "'cifar10': tf.reshape((0.4914, 0.4822, 0.4465), shape=(1, 1, 3)),\n",
        "'cifar100': tf.reshape((0.5071, 0.4867, 0.4408), shape=(1, 1, 3)),\n",
        "}\n",
        "\n",
        "std = {\n",
        "'cifar10': tf.reshape((0.2023, 0.1994, 0.2010), shape=(1, 1, 3)),\n",
        "'cifar100': tf.reshape((0.2675, 0.2565, 0.2761), shape=(1, 1, 3))\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNlFiu2Rzf5C",
        "outputId": "7eff439a-3cb5-487c-afe7-5be647c7818d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "source": [
        "cta = CTAugment(10)\n",
        "#new, choices, bins = cta.augment(image)\n",
        "tf.print(image.shape)\n",
        "new = cta.augment_batch(image)\n",
        "tf.print(new.shape)\n",
        "plt.imshow(new)\n",
        "\n",
        "lab = tf.one_hot(1, 10)\n",
        "pred = tf.random.uniform([10], maxval=1)\n",
        "\n",
        "#cta.update_weights(lab, pred, choices, bins)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-492-2a7102435617>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCTAugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#new, choices, bins = cta.augment(image)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'image' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JAbVTvztHEJ",
        "outputId": "fe75f92c-0547-4791-c7a4-0bc242e39851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        }
      },
      "source": [
        "cta = CTAugment(10)\n",
        "new, choices, bins = cta.augment(image)\n",
        "plt.imshow(new)\n",
        "\n",
        "lab = tf.one_hot(1, 10)\n",
        "pred = tf.random.uniform([10], maxval=1)\n",
        "\n",
        "cta.update_weights(lab, pred, choices, bins)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(10,), dtype=float32)\n",
            "1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-66546c3b6c93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-56-a4268747ab65>\u001b[0m in \u001b[0;36mupdate_weights\u001b[0;34m(self, label, pred, choices, bins)\u001b[0m\n\u001b[1;32m     84\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m       \u001b[0momega\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: return arrays must be of ArrayType"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1xOtqKpuLZ4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}