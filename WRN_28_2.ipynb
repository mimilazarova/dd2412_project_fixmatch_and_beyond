{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled12.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOciJSH13DBDPTTpK7w3MCE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mimilazarova/dd2412_project_fixmatch_and_beyond/blob/main/WRN_28_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXaHCn3SUuY7"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "def regularized_padded_conv(*args, **kwargs):\n",
        "    return tf.keras.layers.Conv2D(*args, **kwargs, padding='same', kernel_regularizer=_regularizer,\n",
        "                                  kernel_initializer='he_normal', use_bias=False)\n",
        "\n",
        "\n",
        "def BN_ReLU(x):\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    return tf.keras.layers.ReLU()(x)\n",
        "\n",
        "\n",
        "def shortcut(x, filters, stride, mode):\n",
        "    if x.shape[-1] == filters:\n",
        "        return x\n",
        "    elif mode == 'B':\n",
        "        return regularized_padded_conv(filters, 1, strides=stride)(x)\n",
        "    elif mode == 'B_original':\n",
        "        x = regularized_padded_conv(filters, 1, strides=stride)(x)\n",
        "        return tf.keras.layers.BatchNormalization()(x)\n",
        "    elif mode == 'A':\n",
        "        return tf.pad(tf.keras.layers.MaxPool2D(1, stride)(x) if stride>1 else x,\n",
        "                      paddings=[(0, 0), (0, 0), (0, 0), (0, filters - x.shape[-1])])\n",
        "    else:\n",
        "        raise KeyError(\"Parameter shortcut_type not recognized!\")\n",
        "    \n",
        "\n",
        "def original_block(x, filters, stride=1, **kwargs):\n",
        "    c1 = regularized_padded_conv(filters, 3, strides=stride)(x)\n",
        "    c2 = regularized_padded_conv(filters, 3)(BN_ReLU(c1))\n",
        "    c2 = tf.keras.layers.BatchNormalization()(c2)\n",
        "    \n",
        "    mode = 'B_original' if _shortcut_type == 'B' else _shortcut_type\n",
        "    x = shortcut(x, filters, stride, mode=mode)\n",
        "    return tf.keras.layers.ReLU()(x + c2)\n",
        "    \n",
        "    \n",
        "def preactivation_block(x, filters, stride=1, preact_block=False):\n",
        "    flow = BN_ReLU(x)\n",
        "    if preact_block:\n",
        "        x = flow\n",
        "        \n",
        "    c1 = regularized_padded_conv(filters, 3, strides=stride)(flow)\n",
        "    if _dropout:\n",
        "        c1 = tf.keras.layers.Dropout(_dropout)(c1)\n",
        "        \n",
        "    c2 = regularized_padded_conv(filters, 3)(BN_ReLU(c1))\n",
        "    x = shortcut(x, filters, stride, mode=_shortcut_type)\n",
        "    return x + c2\n",
        "\n",
        "\n",
        "def bootleneck_block(x, filters, stride=1, preact_block=False):\n",
        "    flow = BN_ReLU(x)\n",
        "    if preact_block:\n",
        "        x = flow\n",
        "         \n",
        "    c1 = regularized_padded_conv(filters//_bootleneck_width, 1)(flow)\n",
        "    c2 = regularized_padded_conv(filters//_bootleneck_width, 3, strides=stride)(BN_ReLU(c1))\n",
        "    c3 = regularized_padded_conv(filters, 1)(BN_ReLU(c2))\n",
        "    x = shortcut(x, filters, stride, mode=_shortcut_type)\n",
        "    return x + c3\n",
        "\n",
        "\n",
        "def group_of_blocks(x, block_type, num_blocks, filters, stride, block_idx=0):\n",
        "    global _preact_shortcuts\n",
        "    preact_block = True if _preact_shortcuts or block_idx == 0 else False\n",
        "    \n",
        "    x = block_type(x, filters, stride, preact_block=preact_block)\n",
        "    for i in range(num_blocks-1):\n",
        "        x = block_type(x, filters)\n",
        "    return x\n",
        "\n",
        "\n",
        "def Resnet(input_shape, n_classes, l2_reg=1e-4, group_sizes=(2, 2, 2), features=(16, 32, 64), strides=(1, 2, 2),\n",
        "           shortcut_type='B', block_type='preactivated', first_conv={\"filters\": 16, \"kernel_size\": 3, \"strides\": 1},\n",
        "           dropout=0, cardinality=1, bootleneck_width=4, preact_shortcuts=True):\n",
        "    \n",
        "    global _regularizer, _shortcut_type, _preact_projection, _dropout, _cardinality, _bootleneck_width, _preact_shortcuts\n",
        "    _bootleneck_width = bootleneck_width # used in ResNeXts and bootleneck blocks\n",
        "    _regularizer = tf.keras.regularizers.l2(l2_reg)\n",
        "    _shortcut_type = shortcut_type # used in blocks\n",
        "    _cardinality = cardinality # used in ResNeXts\n",
        "    _dropout = dropout # used in Wide ResNets\n",
        "    _preact_shortcuts = preact_shortcuts\n",
        "    \n",
        "    block_types = {'preactivated': preactivation_block,\n",
        "                   'bootleneck': bootleneck_block,\n",
        "                   'original': original_block}\n",
        "    \n",
        "    selected_block = block_types[block_type]\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "    flow = regularized_padded_conv(**first_conv)(inputs)\n",
        "    \n",
        "    if block_type == 'original':\n",
        "        flow = BN_ReLU(flow)\n",
        "    \n",
        "    for block_idx, (group_size, feature, stride) in enumerate(zip(group_sizes, features, strides)):\n",
        "        flow = group_of_blocks(flow,\n",
        "                               block_type=selected_block,\n",
        "                               num_blocks=group_size,\n",
        "                               block_idx=block_idx,\n",
        "                               filters=feature,\n",
        "                               stride=stride)\n",
        "    \n",
        "    if block_type != 'original':\n",
        "        flow = BN_ReLU(flow)\n",
        "    \n",
        "    flow = tf.keras.layers.GlobalAveragePooling2D()(flow)\n",
        "    outputs = tf.keras.layers.Dense(n_classes, kernel_regularizer=_regularizer)(flow)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_weights_func(model, model_name):\n",
        "    try: model.load_weights(os.path.join('saved_models', model_name + '.tf'))\n",
        "    except tf.errors.NotFoundError: print(\"No weights found for this model!\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def cifar_wide_resnet(N, K, block_type='preactivated', shortcut_type='B', dropout=0, l2_reg=2.5e-4):\n",
        "    assert (N-4) % 6 == 0, \"N-4 has to be divisible by 6\"\n",
        "    lpb = (N-4) // 6 # layers per block - since N is total number of convolutional layers in Wide ResNet\n",
        "    model = Resnet(input_shape=(32, 32, 3), n_classes=10, l2_reg=l2_reg, group_sizes=(lpb, lpb, lpb), features=(16*K, 32*K, 64*K),\n",
        "                   strides=(1, 2, 2), first_conv={\"filters\": 16, \"kernel_size\": 3, \"strides\": 1}, shortcut_type=shortcut_type,\n",
        "                   block_type=block_type, dropout=dropout, preact_shortcuts=True)\n",
        "    return model\n",
        "\n",
        "\n",
        "def WRN_28_2(shortcut_type='B', load_weights=False, dropout=0, l2_reg=2.5e-4):\n",
        "    model = cifar_wide_resnet(28, 2, 'preactivated', shortcut_type, dropout=dropout, l2_reg=l2_reg)\n",
        "    if load_weights: model = load_weights_func(model, 'cifar_WRN_28_10')\n",
        "    return model\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MY99qVwV6hh"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "logging.getLogger('tensorflow').disabled = True\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "\n",
        "def cifar_training(model, lr_values=[0.01, 0.1, 0.01, 0.001], lr_boundaries=[400, 32000, 48000, 64000],\n",
        "                   val_interval=2000, log_interval=200, batch_size=128, nesterov=False):\n",
        "\n",
        "    schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries=lr_boundaries[:-1], values=lr_values)\n",
        "    optimizer = tf.keras.optimizers.SGD(schedule, momentum=0.9, nesterov=nesterov)\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    ds = tfds.load('cifar10', as_supervised=True, in_memory=True)\n",
        "    std = tf.reshape((0.2023, 0.1994, 0.2010), shape=(1, 1, 3))\n",
        "    mean = tf.reshape((0.4914, 0.4822, 0.4465), shape=(1, 1, 3))\n",
        "    \n",
        "    def train_prep(x, y):\n",
        "        x = tf.cast(x, tf.float32)/255.\n",
        "        x = tf.image.random_flip_left_right(x)\n",
        "        x = tf.image.pad_to_bounding_box(x, 4, 4, 40, 40)\n",
        "        x = tf.image.random_crop(x, (32, 32, 3))\n",
        "        x = (x - mean) / std\n",
        "        return x, y\n",
        "\n",
        "    def valid_prep(x, y):\n",
        "        x = tf.cast(x, tf.float32)/255.\n",
        "        x = (x - mean) / std\n",
        "        return x, y\n",
        "\n",
        "    ds['train'] = ds['train'].map(train_prep).shuffle(10000).repeat().batch(batch_size).prefetch(-1)\n",
        "    ds['test'] = ds['test'].map(valid_prep).batch(batch_size*4).prefetch(-1)\n",
        "\n",
        "    #runid = run_name + '_x' + str(np.random.randint(10000))\n",
        "    #writer = tf.summary.create_file_writer(logdir + '/' + runid)\n",
        "    accuracy = tf.metrics.SparseCategoricalAccuracy()\n",
        "    cls_loss = tf.metrics.Mean()\n",
        "    reg_loss = tf.metrics.Mean()\n",
        "    \n",
        "    #print(f\"RUNID: {runid}\")\n",
        "    tf.keras.utils.plot_model(model)#, os.path.join('saved_plots', runid + '.png'))\n",
        "    \n",
        "    @tf.function\n",
        "    def step(x, y, training):\n",
        "        with tf.GradientTape() as tape:\n",
        "            r_loss = tf.add_n(model.losses)\n",
        "            outs = model(x, training)\n",
        "            c_loss = loss_fn(y, outs)\n",
        "            loss = c_loss + r_loss\n",
        "\n",
        "        if training:\n",
        "            gradients = tape.gradient(loss, model.trainable_weights)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "        accuracy(y, outs)\n",
        "        cls_loss(c_loss)\n",
        "        reg_loss(r_loss)\n",
        "        \n",
        "\n",
        "    training_step = 0\n",
        "    best_validation_acc = 0\n",
        "    #epochs = lr_boundaries[-1] // val_interval\n",
        "    epochs = 3\n",
        "    \n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        for x, y in ds['train'].take(val_interval):\n",
        "\n",
        "            training_step += 1\n",
        "            step(x, y, training=True)\n",
        "\n",
        "            #if training_step % log_interval == 0:\n",
        "                #with writer.as_default():\n",
        "            c_loss, r_loss, err = cls_loss.result(), reg_loss.result(), 1-accuracy.result()\n",
        "            print(f\" c_loss: {c_loss:^6.3f} | r_loss: {r_loss:^6.3f} | err: {err:^6.3f}\", end='\\r')\n",
        "\n",
        "            tf.summary.scalar('train/error_rate', err, training_step)\n",
        "            tf.summary.scalar('train/classification_loss', c_loss, training_step)\n",
        "            tf.summary.scalar('train/regularization_loss', r_loss, training_step)\n",
        "            tf.summary.scalar('train/learnig_rate', optimizer._decayed_lr('float32'), training_step)\n",
        "            cls_loss.reset_states()\n",
        "            reg_loss.reset_states()\n",
        "            accuracy.reset_states()\n",
        "\n",
        "        for x, y in ds['test']:\n",
        "            step(x, y, training=False)\n",
        "\n",
        "        #with writer.as_default(): TBULATE THE FOLLOWING WHEN UNCOMMENTING!\n",
        "        tf.summary.scalar('test/classification_loss', cls_loss.result(), step=training_step)\n",
        "        tf.summary.scalar('test/error_rate', 1-accuracy.result(), step=training_step)\n",
        "            \n",
        "        if accuracy.result() > best_validation_acc:\n",
        "                best_validation_acc = accuracy.result()\n",
        "                #model.save_weights(os.path.join('saved_models', runid + '.tf'))\n",
        "                \n",
        "        cls_loss.reset_states()\n",
        "        accuracy.reset_states()\n",
        "            \n",
        "    \n",
        "    \n",
        "def cifar_error_test(model, tr_len=20, vd_len=2):\n",
        "\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    optimizer = tf.keras.optimizers.SGD(0.01)\n",
        "\n",
        "    ds = tfds.load('cifar10', as_supervised=True, in_memory=True)\n",
        "    std = tf.reshape((0.2023, 0.1994, 0.2010), shape=(1, 1, 3))\n",
        "    mean= tf.reshape((0.4914, 0.4822, 0.4465), shape=(1, 1, 3))\n",
        "    \n",
        "    def train_prep(x, y):\n",
        "        x = tf.cast(x, tf.float32)/255.\n",
        "        x = tf.image.random_flip_left_right(x)\n",
        "        x = tf.image.pad_to_bounding_box(x, 4, 4, 40, 40)\n",
        "        x = tf.image.random_crop(x, (32, 32, 3))\n",
        "        x = (x - mean) / std\n",
        "        return x, y\n",
        "\n",
        "    def valid_prep(x, y):\n",
        "        x = tf.cast(x, tf.float32)/255.\n",
        "        x = (x - mean) / std\n",
        "        return x, y\n",
        "\n",
        "    ds['train'] = ds['train'].map(train_prep).batch(5).take(tr_len).prefetch(-1)\n",
        "    ds['test'] = ds['test'].map(valid_prep).batch(5).take(vd_len).prefetch(-1)\n",
        "\n",
        "    accuracy = tf.metrics.SparseCategoricalAccuracy()\n",
        "    cls_loss = tf.metrics.Mean()\n",
        "    reg_loss = tf.metrics.Mean()\n",
        "\n",
        "    @tf.function\n",
        "    def step(x, y, training):\n",
        "        with tf.GradientTape() as tape:\n",
        "            r_loss = tf.add_n(model.losses)\n",
        "            outs = model(x, training)\n",
        "            c_loss = loss_fn(y, outs)\n",
        "            loss = c_loss + r_loss\n",
        "\n",
        "        if training:\n",
        "            gradients = tape.gradient(loss, model.trainable_weights)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "        accuracy(y, outs)\n",
        "        cls_loss(c_loss)\n",
        "        reg_loss(r_loss)\n",
        "        \n",
        "    training_step = 0\n",
        "    for x, y in tqdm(ds['train'], desc=f'test', total=tr_len, ncols=100, ascii=True):\n",
        "\n",
        "        training_step += 1\n",
        "        step(x, y, training=True)\n",
        "        c_loss, r_loss, err = cls_loss.result(), reg_loss.result(), 1-accuracy.result()\n",
        "        print(f\" c_loss: {c_loss:^6.3f} | r_loss: {r_loss:^6.3f} | err: {err:^6.3f}\", end='\\r')\n",
        "\n",
        "    for x, y in ds['test']:\n",
        "        step(x, y, training=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h06AqVwlXATf"
      },
      "source": [
        "model = WRN_28_2()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIpVOZA3V7Wl",
        "outputId": "fe7086eb-5af0-40ac-ab0f-bf670050c878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "source": [
        "cifar_training(model)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:`in_memory` if deprecated and will be removed in a future version. Please use `ds = ds.cache()` instead.\n",
            "WARNING:absl:`in_memory` if deprecated and will be removed in a future version. Please use `ds = ds.cache()` instead.\n",
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 33%|███▎      | 1/3 [02:47<05:35, 167.87s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 67%|██████▋   | 2/3 [05:32<02:46, 166.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 3/3 [08:16<00:00, 165.59s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB9O_Ae7zDku",
        "outputId": "468f8261-1aba-41ca-993b-3ee1fe99898c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "source": [
        "cifar_error_test(model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:`in_memory` if deprecated and will be removed in a future version. Please use `ds = ds.cache()` instead.\n",
            "WARNING:absl:`in_memory` if deprecated and will be removed in a future version. Please use `ds = ds.cache()` instead.\n",
            "\n",
            "test:   0%|                                                                  | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "test:   5%|##9                                                       | 1/20 [00:01<00:36,  1.90s/it]\u001b[A\n",
            "test:  50%|############################5                            | 10/20 [00:02<00:13,  1.33s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " c_loss: 1.168  | r_loss: 0.297  | err: 0.400 \r c_loss: 0.889  | r_loss: 0.297  | err: 0.300 \r c_loss: 0.820  | r_loss: 0.297  | err: 0.267 \r c_loss: 0.884  | r_loss: 0.297  | err: 0.300 \r c_loss: 0.759  | r_loss: 0.297  | err: 0.240 \r c_loss: 1.232  | r_loss: 0.297  | err: 0.267 \r c_loss: 1.176  | r_loss: 0.297  | err: 0.286 \r c_loss: 1.036  | r_loss: 0.297  | err: 0.250 \r c_loss: 1.021  | r_loss: 0.297  | err: 0.244 \r c_loss: 1.040  | r_loss: 0.297  | err: 0.260 \r c_loss: 1.051  | r_loss: 0.297  | err: 0.291 \r c_loss: 1.046  | r_loss: 0.297  | err: 0.283 \r c_loss: 0.983  | r_loss: 0.297  | err: 0.262 \r c_loss: 1.008  | r_loss: 0.297  | err: 0.271 \r c_loss: 0.992  | r_loss: 0.297  | err: 0.267 \r c_loss: 0.949  | r_loss: 0.297  | err: 0.250 \r c_loss: 0.898  | r_loss: 0.297  | err: 0.235 \r c_loss: 0.968  | r_loss: 0.297  | err: 0.256 \r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "test: 100%|#########################################################| 20/20 [00:02<00:00,  9.44it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " c_loss: 0.931  | r_loss: 0.297  | err: 0.253 \r c_loss: 0.890  | r_loss: 0.297  | err: 0.240 \r"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}